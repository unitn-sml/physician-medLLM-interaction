{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "!pip install sacrebleu\n",
    "!pip install rouge_score\n",
    "!pip install bert_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import evaluate\n",
    "import sacrebleu\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'add_your_path'\n",
    "dpath = path+ \"add_name_of_the_output_file.json\"\n",
    "df = pd.read_json(dpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocessing model outputs\n",
    "\n",
    "## CODE FOR MEDITRON\n",
    "completions = list(df['completion'])\n",
    "\n",
    "completion_short = []\n",
    "completion_long = []\n",
    "i = 0\n",
    "deleted_rows_count = 0\n",
    "for comp in completions:\n",
    "    if (\"Explanation\" in comp) or (\"Explanition\" in comp):\n",
    "        ans = comp.split(\"{\")[2].split(\"}\")[0].replace(\"''\", '').replace(\"'\", '').lstrip(' ').lower()\n",
    "        if ans not in ['yes', 'no']:\n",
    "            #print(f\"Misstructured short answer. Removing the {i}th row...\")\n",
    "            df = df.drop(i)\n",
    "            deleted_rows_count = deleted_rows_count + 1\n",
    "        else:\n",
    "            completion_short.append(ans)\n",
    "            exp = comp.split(\"{\")[3].split(\"}\")[0].replace(\"''\", '').replace(\"'\", '').lstrip(' ').replace(\"<unk>\", \"\").replace(\"</s>\", \"\")\n",
    "            if exp and exp[0] == exp[-1] == \"'\":\n",
    "                exp = exp[1:-1]\n",
    "            completion_long.append(exp)\n",
    "    else:\n",
    "        #print(f\"Misstructured long answer. Removing the {i}th row...\")\n",
    "        df = df.drop(i)\n",
    "        deleted_rows_count = deleted_rows_count + 1\n",
    "    i = i + 1\n",
    "print(\"Total number of deleted rows: \", deleted_rows_count)\n",
    "\n",
    "df[\"short_completion\"] = completion_short\n",
    "df[\"long_completion\"] = completion_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = df[\"final_decision\"].map({'yes': 1, 'no': 0}).to_list()\n",
    "comp_answer = df[\"short_completion\"].map({'yes': 1, 'no': 0}).to_list()\n",
    "long_answer = df[\"LONG_ANSWER\"].to_list()\n",
    "comp_long_answer = df[\"long_completion\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(gt, comp_answer)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(refs, preds):\n",
    "    \"\"\"\n",
    "    Returns `t5` style BLEU scores. See the related implementation:\n",
    "    https://github.com/google-research/text-to-text-transfer-transformer/blob/3d10afd51ba97ac29eb66ae701eca274488202f7/t5/evaluation/metrics.py#L41\n",
    "    :param refs:\n",
    "        A `list` of `list` of reference `str`s.\n",
    "    :param preds:\n",
    "        A `list` of predicted `str`s.\n",
    "    \"\"\"\n",
    "    score = sacrebleu.corpus_bleu(preds, refs, smooth_method=\"exp\", smooth_value=0.0, force=False, lowercase=False, tokenize=\"intl\", use_effective_order=False).score\n",
    "    return score\n",
    "\n",
    "\n",
    "def rouge(refs, preds):\n",
    "    \"\"\"\n",
    "    Returns `t5` style ROUGE scores. See the related implementation:\n",
    "    https://github.com/google-research/text-to-text-transfer-transformer/blob/3d10afd51ba97ac29eb66ae701eca274488202f7/t5/evaluation/metrics.py#L68\n",
    "    :param refs:\n",
    "        A `list` of reference `strs`.\n",
    "    :param preds:\n",
    "        A `list` of predicted `strs`.\n",
    "    \"\"\"\n",
    "    rouge_types = [\"rouge1\", \"rouge2\", \"rougeLsum\"]\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_types)\n",
    "    # Add newlines between sentences to correctly compute `rougeLsum`.\n",
    "\n",
    "    def _prepare_summary(summary):\n",
    "        summary = summary.replace(\" . \", \".\\n\")\n",
    "        return summary\n",
    "\n",
    "    # Accumulate confidence intervals.\n",
    "    aggregator = scoring.BootstrapAggregator()\n",
    "    for ref, pred in zip(refs, preds):\n",
    "        ref = _prepare_summary(ref)\n",
    "        pred = _prepare_summary(pred)\n",
    "        aggregator.add_scores(scorer.score(ref, pred))\n",
    "    result = aggregator.aggregate()\n",
    "    return {type: result[type].mid.fmeasure * 100 for type in rouge_types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE-N\n",
    "rouge_scores = [rouge([ref], comp_long_answer) for ref in long_answer]\n",
    "\n",
    "# ROUGE-L\n",
    "rougeL_scores = [score[\"rougeLsum\"] for score in rouge_scores]\n",
    "print(\"rougeL_scores: \", rougeL_scores[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
