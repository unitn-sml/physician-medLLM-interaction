{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "!pip install sacrebleu\n",
    "!pip install rouge_score\n",
    "!pip install bert_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import evaluate\n",
    "import sacrebleu\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'add_your_path'\n",
    "dpath = path+ \"add_name_of_the_output_file.json\"\n",
    "df = pd.read_json(dpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocessing model outputs\n",
    "\n",
    "completions = list(df['completion'])\n",
    "indices_to_delete = []\n",
    "completion_short = []\n",
    "completion_long = []\n",
    "for i in range(len(completions)):\n",
    "    comp = completions[i]\n",
    "    if \"Answer:\" in comp:\n",
    "        completion_short.append(comp.split(\"Answer:\")[1].lstrip(' ').split(\"\\n\")[0].lstrip(' ').lower())\n",
    "        completion_long.append(comp.split(\"Explanation:\")[1].lstrip(' ').split(\"\\n\")[0].lstrip(' ').lower()).replace(\"<unk>\", \"\").replace(\"</s>\", \"\")\n",
    "    else:\n",
    "        indices_to_delete.append(i)\n",
    "\n",
    "print(\"num of ind to delete: \", indices_to_delete)\n",
    "for j in range(len(indices_to_delete)):\n",
    "    df = df.drop(indices_to_delete[j])\n",
    "\n",
    "df[\"short_completion\"] = completion_short\n",
    "df[\"long_completion\"] = completion_long\n",
    "\n",
    "df['short_completion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = df[\"final_decision\"].map({'yes': 1, 'no': 0}).to_list()\n",
    "comp_answer = df[\"short_completion\"].map({'yes': 1, 'no': 0}).to_list()\n",
    "long_answer = df[\"LONG_ANSWER\"].to_list()\n",
    "comp_long_answer = df[\"long_completion\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(gt, comp_answer)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(refs, preds):\n",
    "    \"\"\"\n",
    "    Returns `t5` style BLEU scores. See the related implementation:\n",
    "    https://github.com/google-research/text-to-text-transfer-transformer/blob/3d10afd51ba97ac29eb66ae701eca274488202f7/t5/evaluation/metrics.py#L41\n",
    "    :param refs:\n",
    "        A `list` of `list` of reference `str`s.\n",
    "    :param preds:\n",
    "        A `list` of predicted `str`s.\n",
    "    \"\"\"\n",
    "    score = sacrebleu.corpus_bleu(preds, refs, smooth_method=\"exp\", smooth_value=0.0, force=False, lowercase=False, tokenize=\"intl\", use_effective_order=False).score\n",
    "    return score\n",
    "\n",
    "\n",
    "def rouge(refs, preds):\n",
    "    \"\"\"\n",
    "    Returns `t5` style ROUGE scores. See the related implementation:\n",
    "    https://github.com/google-research/text-to-text-transfer-transformer/blob/3d10afd51ba97ac29eb66ae701eca274488202f7/t5/evaluation/metrics.py#L68\n",
    "    :param refs:\n",
    "        A `list` of reference `strs`.\n",
    "    :param preds:\n",
    "        A `list` of predicted `strs`.\n",
    "    \"\"\"\n",
    "    rouge_types = [\"rouge1\", \"rouge2\", \"rougeLsum\"]\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_types)\n",
    "    # Add newlines between sentences to correctly compute `rougeLsum`.\n",
    "\n",
    "    def _prepare_summary(summary):\n",
    "        summary = summary.replace(\" . \", \".\\n\")\n",
    "        return summary\n",
    "\n",
    "    # Accumulate confidence intervals.\n",
    "    aggregator = scoring.BootstrapAggregator()\n",
    "    for ref, pred in zip(refs, preds):\n",
    "        ref = _prepare_summary(ref)\n",
    "        pred = _prepare_summary(pred)\n",
    "        aggregator.add_scores(scorer.score(ref, pred))\n",
    "    result = aggregator.aggregate()\n",
    "    return {type: result[type].mid.fmeasure * 100 for type in rouge_types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE-N\n",
    "rouge_scores = [rouge([ref], comp_long_answer) for ref in long_answer]\n",
    "\n",
    "# ROUGE-L\n",
    "rougeL_scores = [score[\"rougeLsum\"] for score in rouge_scores]\n",
    "print(\"rougeL_scores: \", rougeL_scores[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
